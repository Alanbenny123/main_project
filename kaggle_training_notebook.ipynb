{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Student Behavior Analysis & Face Recognition Training\n",
        "\n",
        "This notebook trains two models:\n",
        "1. **Behavior Classification** (Swin Transformer)\n",
        "2. **Face Recognition** (ArcFace + ResNet50)\n",
        "\n",
        "## Hardware: Kaggle GPU (P100) - Optimized Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Setup Instructions\n",
        "\n",
        "**Before running this notebook:**\n",
        "1. Upload your dataset as a Kaggle Dataset or zip file\n",
        "2. Enable GPU: Settings ‚Üí Accelerator ‚Üí GPU P100\n",
        "3. Enable Internet: Settings ‚Üí Internet ‚Üí ON (for downloading pretrained models)\n",
        "\n",
        "**Dataset structure expected:**\n",
        "```\n",
        "Behaviors_Features/\n",
        "‚îú‚îÄ‚îÄ Looking_Forward/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ ID1/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ ID2/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ ID3/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ID4/\n",
        "‚îú‚îÄ‚îÄ Raising_Hand/\n",
        "‚îú‚îÄ‚îÄ Reading/\n",
        "‚îú‚îÄ‚îÄ Sleeping/\n",
        "‚îú‚îÄ‚îÄ Standing/\n",
        "‚îú‚îÄ‚îÄ Turning_Around/\n",
        "‚îî‚îÄ‚îÄ Writting/\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install timm seaborn pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import random\n",
        "import json\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Dataset Setup\n",
        "\n",
        "Upload your dataset and adjust the path below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== ADJUST THIS PATH =====\n",
        "# Option 1: If you uploaded as Kaggle Dataset\n",
        "DATASET_PATH = \"/kaggle/input/your-dataset-name/Behaviors_Features\"\n",
        "\n",
        "# Option 2: If you uploaded a zip file, uncomment below:\n",
        "# !unzip -q /kaggle/input/your-zip-file/Behaviors_Features.zip -d /kaggle/working/\n",
        "# DATASET_PATH = \"/kaggle/working/Behaviors_Features\"\n",
        "\n",
        "# Verify dataset exists\n",
        "if Path(DATASET_PATH).exists():\n",
        "    print(f\"‚úÖ Dataset found at: {DATASET_PATH}\")\n",
        "    print(f\"   Classes: {[d.name for d in Path(DATASET_PATH).iterdir() if d.is_dir()]}\")\n",
        "else:\n",
        "    print(f\"‚ùå Dataset NOT found at: {DATASET_PATH}\")\n",
        "    print(\"   Please adjust DATASET_PATH above!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóÇÔ∏è Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BehaviorDataset(Dataset):\n",
        "    \"\"\"Student Behavior Classification Dataset\"\"\"\n",
        "    \n",
        "    CLASSES = [\n",
        "        'Looking_Forward',\n",
        "        'Raising_Hand',\n",
        "        'Reading',\n",
        "        'Sleeping',\n",
        "        'Standing',\n",
        "        'Turning_Around',\n",
        "        'Writting'\n",
        "    ]\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        root_dir: str,\n",
        "        student_ids: List[str] = None,\n",
        "        transform: Optional[transforms.Compose] = None,\n",
        "        augment: bool = True,\n",
        "        max_samples_per_class: Optional[int] = None\n",
        "    ):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.student_ids = student_ids or ['ID1', 'ID2', 'ID3', 'ID4']\n",
        "        self.max_samples_per_class = max_samples_per_class\n",
        "        \n",
        "        # Class mapping\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.CLASSES)}\n",
        "        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}\n",
        "        \n",
        "        # Transforms\n",
        "        if transform:\n",
        "            self.transform = transform\n",
        "        else:\n",
        "            self.transform = self._get_default_transforms(augment)\n",
        "        \n",
        "        # Load samples\n",
        "        self.samples = self._load_samples()\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples):,} samples from {len(self.student_ids)} students\")\n",
        "        self._print_class_distribution()\n",
        "    \n",
        "    def _get_default_transforms(self, augment: bool) -> transforms.Compose:\n",
        "        \"\"\"Default transforms for Swin Transformer\"\"\"\n",
        "        if augment:\n",
        "            return transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.RandomCrop(224),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomRotation(10),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            return transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "    \n",
        "    def _load_samples(self) -> List[Tuple[Path, int]]:\n",
        "        \"\"\"Load all image paths and labels\"\"\"\n",
        "        samples = []\n",
        "        \n",
        "        for class_name in self.CLASSES:\n",
        "            class_dir = self.root_dir / class_name\n",
        "            if not class_dir.exists():\n",
        "                print(f\"‚ö†Ô∏è Warning: {class_dir} not found, skipping\")\n",
        "                continue\n",
        "            \n",
        "            class_samples = []\n",
        "            class_idx = self.class_to_idx[class_name]\n",
        "            \n",
        "            for student_id in self.student_ids:\n",
        "                student_dir = class_dir / student_id\n",
        "                if not student_dir.exists():\n",
        "                    continue\n",
        "                \n",
        "                png_files = list(student_dir.rglob('*.png'))\n",
        "                for img_path in png_files:\n",
        "                    class_samples.append((img_path, class_idx))\n",
        "            \n",
        "            # Limit samples if specified\n",
        "            if self.max_samples_per_class and len(class_samples) > self.max_samples_per_class:\n",
        "                class_samples = random.sample(class_samples, self.max_samples_per_class)\n",
        "            \n",
        "            samples.extend(class_samples)\n",
        "        \n",
        "        random.shuffle(samples)\n",
        "        return samples\n",
        "    \n",
        "    def _print_class_distribution(self):\n",
        "        \"\"\"Print class statistics\"\"\"\n",
        "        class_counts = {}\n",
        "        for _, label in self.samples:\n",
        "            class_name = self.idx_to_class[label]\n",
        "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
        "        \n",
        "        print(\"\\nüìä Class Distribution:\")\n",
        "        print(\"-\" * 60)\n",
        "        for class_name in self.CLASSES:\n",
        "            count = class_counts.get(class_name, 0)\n",
        "            pct = (count / len(self.samples) * 100) if self.samples else 0\n",
        "            bar = '‚ñà' * int(pct / 2)\n",
        "            print(f\"{class_name:<20} {count:>8,} ({pct:>5.2f}%) {bar}\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        \n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "    \n",
        "    def get_class_weights(self):\n",
        "        \"\"\"Calculate class weights for imbalanced data\"\"\"\n",
        "        class_counts = torch.zeros(len(self.CLASSES))\n",
        "        for _, label in self.samples:\n",
        "            class_counts[label] += 1\n",
        "        \n",
        "        total = len(self.samples)\n",
        "        weights = total / (len(self.CLASSES) * class_counts)\n",
        "        weights = weights / weights.sum() * len(self.CLASSES)\n",
        "        return weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BehaviorClassifier(nn.Module):\n",
        "    \"\"\"Swin Transformer for Behavior Classification\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=7, pretrained=True, model_name='swin_tiny_patch4_window7_224'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        self.model_name = model_name\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "print(\"‚úÖ Model class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== CONFIGURATION =====\n",
        "CONFIG = {\n",
        "    # Training\n",
        "    'batch_size': 64,           # P100 has 16GB VRAM, can handle large batches\n",
        "    'num_epochs': 25,           # Full training\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 1e-4,\n",
        "    \n",
        "    # Model\n",
        "    'model_name': 'swin_tiny_patch4_window7_224',\n",
        "    'pretrained': True,\n",
        "    'use_class_weights': True,\n",
        "    \n",
        "    # Data\n",
        "    'num_workers': 2,           # Kaggle works well with 2 workers\n",
        "    'train_ids': ['ID1', 'ID2', 'ID3'],\n",
        "    'val_ids': ['ID4'],\n",
        "    'test_ids': ['ID4'],\n",
        "    \n",
        "    # Device\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \n",
        "    # Paths\n",
        "    'save_dir': '/kaggle/working/models'\n",
        "}\n",
        "\n",
        "print(\"üìù Configuration:\")\n",
        "for key, val in CONFIG.items():\n",
        "    print(f\"   {key:<20} {val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Load Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîÑ Creating datasets...\")\n",
        "\n",
        "train_dataset = BehaviorDataset(\n",
        "    root_dir=DATASET_PATH,\n",
        "    student_ids=CONFIG['train_ids'],\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "val_dataset = BehaviorDataset(\n",
        "    root_dir=DATASET_PATH,\n",
        "    student_ids=CONFIG['val_ids'],\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "test_dataset = BehaviorDataset(\n",
        "    root_dir=DATASET_PATH,\n",
        "    student_ids=CONFIG['test_ids'],\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataloaders created:\")\n",
        "print(f\"   Train batches: {len(train_loader)}\")\n",
        "print(f\"   Val batches:   {len(val_loader)}\")\n",
        "print(f\"   Test batches:  {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîÑ Creating Swin Transformer model...\")\n",
        "\n",
        "model = BehaviorClassifier(\n",
        "    num_classes=7,\n",
        "    pretrained=CONFIG['pretrained'],\n",
        "    model_name=CONFIG['model_name']\n",
        ")\n",
        "model = model.to(CONFIG['device'])\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"‚úÖ Model created:\")\n",
        "print(f\"   Architecture: {CONFIG['model_name']}\")\n",
        "print(f\"   Total params: {total_params:,}\")\n",
        "print(f\"   Trainable:    {trainable_params:,}\")\n",
        "\n",
        "# Loss function with class weights\n",
        "if CONFIG['use_class_weights']:\n",
        "    class_weights = train_dataset.get_class_weights().to(CONFIG['device'])\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    print(f\"\\nüéØ Using weighted loss:\")\n",
        "    for i, weight in enumerate(class_weights):\n",
        "        print(f\"   {train_dataset.idx_to_class[i]:<20} {weight:.4f}\")\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(\"\\nüéØ Using standard CrossEntropyLoss\")\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay']\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Optimizer: AdamW (lr={CONFIG['learning_rate']}, wd={CONFIG['weight_decay']})\")\n",
        "print(f\"‚úÖ Scheduler: ReduceLROnPlateau (patience=3)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc=\"Training\")\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        total += images.size(0)\n",
        "        \n",
        "        # Update progress bar\n",
        "        acc = running_corrects.double() / total\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{acc:.4f}'})\n",
        "    \n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = running_corrects.double() / total\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, criterion, device):\n",
        "    \"\"\"Validate model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(loader, desc=\"Validation\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        total += images.size(0)\n",
        "    \n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = running_corrects.double() / total\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "print(\"‚úÖ Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create save directory\n",
        "Path(CONFIG['save_dir']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': [],\n",
        "    'lr': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_path = None\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Device: {CONFIG['device']}\")\n",
        "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
        "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(CONFIG['num_epochs']):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, CONFIG['device'])\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, CONFIG['device'])\n",
        "    \n",
        "    # Update scheduler\n",
        "    scheduler.step(val_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    \n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['lr'].append(current_lr)\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "    print(f\"\\nüìä Results:\")\n",
        "    print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"   Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
        "    print(f\"   LR: {current_lr:.6f} | Time: {epoch_time:.1f}s\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_path = Path(CONFIG['save_dir']) / 'best_behavior_model.pth'\n",
        "        \n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'history': history,\n",
        "            'config': CONFIG\n",
        "        }, best_model_path)\n",
        "        \n",
        "        print(f\"   ‚úÖ Saved new best model (acc: {val_acc:.4f})\")\n",
        "    \n",
        "    # Save checkpoint every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        checkpoint_path = Path(CONFIG['save_dir']) / f'checkpoint_epoch{epoch + 1}.pth'\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'history': history,\n",
        "            'config': CONFIG\n",
        "        }, checkpoint_path)\n",
        "        print(f\"   üíæ Checkpoint saved: epoch {epoch + 1}\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total time: {total_time / 60:.1f} minutes\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"Best model: {best_model_path}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save history\n",
        "history_path = Path(CONFIG['save_dir']) / 'training_history.json'\n",
        "with open(history_path, 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "print(f\"\\nüíæ Training history saved to: {history_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Plot Training Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(epochs_range, history['train_loss'], 'bo-', label='Train', linewidth=2)\n",
        "axes[0].plot(epochs_range, history['val_loss'], 'ro-', label='Validation', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(epochs_range, history['train_acc'], 'bo-', label='Train', linewidth=2)\n",
        "axes[1].plot(epochs_range, history['val_acc'], 'ro-', label='Validation', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[1].set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate plot\n",
        "axes[2].plot(epochs_range, history['lr'], 'go-', linewidth=2)\n",
        "axes[2].set_xlabel('Epoch', fontsize=12)\n",
        "axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
        "axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "axes[2].set_yscale('log')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(CONFIG['save_dir']) / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Training curves saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Test Set Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîÑ Evaluating on test set...\")\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# Collect predictions\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        images = images.to(CONFIG['device'])\n",
        "        outputs = model(images)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        \n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    target_names=BehaviorDataset.CLASSES,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà TEST SET RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Class':<20} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}\")\n",
        "print(\"-\"*80)\n",
        "for class_name in BehaviorDataset.CLASSES:\n",
        "    metrics = report[class_name]\n",
        "    print(f\"{class_name:<20} {metrics['precision']:>10.4f} {metrics['recall']:>10.4f} \"\n",
        "          f\"{metrics['f1-score']:>10.4f} {metrics['support']:>10.0f}\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(f\"{'OVERALL ACCURACY':<20} {report['accuracy']:>10.4f}\")\n",
        "print(f\"{'Macro Avg F1':<20} {report['macro avg']['f1-score']:>10.4f}\")\n",
        "print(f\"{'Weighted Avg F1':<20} {report['weighted avg']['f1-score']:>10.4f}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=BehaviorDataset.CLASSES,\n",
        "    yticklabels=BehaviorDataset.CLASSES,\n",
        "    cbar_kws={'label': 'Count'},\n",
        "    square=True\n",
        ")\n",
        "plt.xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=13, fontweight='bold')\n",
        "plt.title('Confusion Matrix - Test Set', fontsize=15, fontweight='bold', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(CONFIG['save_dir']) / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Confusion matrix saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Save Test Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results = {\n",
        "    'report': report,\n",
        "    'confusion_matrix': cm.tolist(),\n",
        "    'config': CONFIG,\n",
        "    'best_val_acc': best_val_acc\n",
        "}\n",
        "\n",
        "results_path = Path(CONFIG['save_dir']) / 'test_results.json'\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(test_results, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Test results saved to: {results_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• Download Trained Model\n",
        "\n",
        "**Your trained model is saved in `/kaggle/working/models/`**\n",
        "\n",
        "Files to download:\n",
        "- `best_behavior_model.pth` - Best model checkpoint\n",
        "- `training_history.json` - Training metrics\n",
        "- `test_results.json` - Test set evaluation\n",
        "- `training_curves.png` - Training visualizations\n",
        "- `confusion_matrix.png` - Confusion matrix\n",
        "\n",
        "To download:\n",
        "1. Click the \"Output\" tab on the right sidebar\n",
        "2. Click download on the files you want\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all saved files\n",
        "print(\"\\nüìÅ Files in output directory:\")\n",
        "print(\"-\" * 60)\n",
        "for file_path in sorted(Path(CONFIG['save_dir']).glob('*')):\n",
        "    size_mb = file_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"   {file_path.name:<40} {size_mb:>8.2f} MB\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "print(\"\\n‚úÖ ALL DONE! Ready to download and use in your local system.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
